<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Fine Tuning | Generative AI with Large Language Models</title>
  <meta name="description" content="This is a collection of notes from open sources" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Fine Tuning | Generative AI with Large Language Models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a collection of notes from open sources" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Fine Tuning | Generative AI with Large Language Models" />
  
  <meta name="twitter:description" content="This is a collection of notes from open sources" />
  

<meta name="author" content="DEA" />


<meta name="date" content="2025-12-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fine-tuning-llms-with-instructions.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Generative AI Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-1.html"><a href="introduction-1.html#level-1-genai-llms-basics"><i class="fa fa-check"></i><b>1.1</b> <strong>Level 1: GenAI &amp; LLMs Basics</strong></a></li>
<li class="chapter" data-level="1.2" data-path="introduction-1.html"><a href="introduction-1.html#level-2-ai-agent-development"><i class="fa fa-check"></i><b>1.2</b> <strong>Level 2: AI Agent Development</strong></a></li>
<li class="chapter" data-level="1.3" data-path="introduction-1.html"><a href="introduction-1.html#level-3-ai-agents-in-production-multi-agents"><i class="fa fa-check"></i><b>1.3</b> <strong>Level 3: AI Agents in Production + Multi-Agents</strong></a></li>
<li class="chapter" data-level="1.4" data-path="introduction-1.html"><a href="introduction-1.html#generative-ai-overview"><i class="fa fa-check"></i><b>1.4</b> Generative AI Overview</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="introduction-1.html"><a href="introduction-1.html#what-is-generative-ai"><i class="fa fa-check"></i><b>1.4.1</b> What is Generative AI?</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction-1.html"><a href="introduction-1.html#understanding-parameters"><i class="fa fa-check"></i><b>1.4.2</b> Understanding Parameters</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction-1.html"><a href="introduction-1.html#model-adaptation-and-use"><i class="fa fa-check"></i><b>1.4.3</b> Model Adaptation and Use</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction-1.html"><a href="introduction-1.html#interaction-with-llms"><i class="fa fa-check"></i><b>1.4.4</b> Interaction with LLMs</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction-1.html"><a href="introduction-1.html#use-cases-of-llm"><i class="fa fa-check"></i><b>1.5</b> Use Cases of LLM</a></li>
<li class="chapter" data-level="1.6" data-path="introduction-1.html"><a href="introduction-1.html#text-generation-before-transformers"><i class="fa fa-check"></i><b>1.6</b> Text Generation before Transformers</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="introduction-1.html"><a href="introduction-1.html#recurrent-neural-networks-rnns"><i class="fa fa-check"></i><b>1.6.1</b> Recurrent Neural Networks (RNNs)</a></li>
<li class="chapter" data-level="1.6.2" data-path="introduction-1.html"><a href="introduction-1.html#example-of-rnn-in-action"><i class="fa fa-check"></i><b>1.6.2</b> Example of RNN in Action</a></li>
<li class="chapter" data-level="1.6.3" data-path="introduction-1.html"><a href="introduction-1.html#challenges-in-language-understanding"><i class="fa fa-check"></i><b>1.6.3</b> Challenges in Language Understanding</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introduction-1.html"><a href="introduction-1.html#introduction-of-transformer-architecture"><i class="fa fa-check"></i><b>1.7</b> Introduction of Transformer Architecture</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="introduction-1.html"><a href="introduction-1.html#advantages-of-transformers"><i class="fa fa-check"></i><b>1.7.1</b> Advantages of Transformers</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction-1.html"><a href="introduction-1.html#transformers"><i class="fa fa-check"></i><b>1.8</b> Transformers</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="introduction-1.html"><a href="introduction-1.html#overview"><i class="fa fa-check"></i><b>1.8.1</b> Overview</a></li>
<li class="chapter" data-level="1.8.2" data-path="introduction-1.html"><a href="introduction-1.html#attention-mechanisms"><i class="fa fa-check"></i><b>1.8.2</b> Attention Mechanisms</a></li>
<li class="chapter" data-level="1.8.3" data-path="introduction-1.html"><a href="introduction-1.html#transformer-architecture"><i class="fa fa-check"></i><b>1.8.3</b> Transformer Architecture</a></li>
<li class="chapter" data-level="1.8.4" data-path="introduction-1.html"><a href="introduction-1.html#tokenization"><i class="fa fa-check"></i><b>1.8.4</b> Tokenization</a></li>
<li class="chapter" data-level="1.8.5" data-path="introduction-1.html"><a href="introduction-1.html#embedding-layer"><i class="fa fa-check"></i><b>1.8.5</b> Embedding Layer</a></li>
<li class="chapter" data-level="1.8.6" data-path="introduction-1.html"><a href="introduction-1.html#positional-encoding"><i class="fa fa-check"></i><b>1.8.6</b> Positional Encoding</a></li>
<li class="chapter" data-level="1.8.7" data-path="introduction-1.html"><a href="introduction-1.html#self-attention-layer"><i class="fa fa-check"></i><b>1.8.7</b> Self-Attention Layer</a></li>
<li class="chapter" data-level="1.8.8" data-path="introduction-1.html"><a href="introduction-1.html#feed-forward-network"><i class="fa fa-check"></i><b>1.8.8</b> Feed-Forward Network</a></li>
<li class="chapter" data-level="1.8.9" data-path="introduction-1.html"><a href="introduction-1.html#softmax-layer"><i class="fa fa-check"></i><b>1.8.9</b> Softmax Layer</a></li>
<li class="chapter" data-level="1.8.10" data-path="introduction-1.html"><a href="introduction-1.html#prediction-process"><i class="fa fa-check"></i><b>1.8.10</b> Prediction Process</a></li>
<li class="chapter" data-level="1.8.11" data-path="introduction-1.html"><a href="introduction-1.html#example"><i class="fa fa-check"></i><b>1.8.11</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="introduction-1.html"><a href="introduction-1.html#transformer-architecture-1"><i class="fa fa-check"></i><b>1.9</b> Transformer Architecture</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="introduction-1.html"><a href="introduction-1.html#overview-1"><i class="fa fa-check"></i><b>1.9.1</b> Overview</a></li>
<li class="chapter" data-level="1.9.2" data-path="introduction-1.html"><a href="introduction-1.html#encoder"><i class="fa fa-check"></i><b>1.9.2</b> Encoder</a></li>
<li class="chapter" data-level="1.9.3" data-path="introduction-1.html"><a href="introduction-1.html#decoder"><i class="fa fa-check"></i><b>1.9.3</b> Decoder</a></li>
<li class="chapter" data-level="1.9.4" data-path="introduction-1.html"><a href="introduction-1.html#encoder-decoder-models"><i class="fa fa-check"></i><b>1.9.4</b> Encoder-Decoder Models</a></li>
<li class="chapter" data-level="1.9.5" data-path="introduction-1.html"><a href="introduction-1.html#practical-application"><i class="fa fa-check"></i><b>1.9.5</b> Practical Application</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="introduction-1.html"><a href="introduction-1.html#some-well-known-models"><i class="fa fa-check"></i><b>1.10</b> Some well-known models</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="introduction-1.html"><a href="introduction-1.html#bert"><i class="fa fa-check"></i><b>1.10.1</b> BERT</a></li>
<li class="chapter" data-level="1.10.2" data-path="introduction-1.html"><a href="introduction-1.html#bloom"><i class="fa fa-check"></i><b>1.10.2</b> BLOOM</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="introduction-1.html"><a href="introduction-1.html#prompt-engineering"><i class="fa fa-check"></i><b>1.11</b> Prompt Engineering</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="introduction-1.html"><a href="introduction-1.html#key-concepts"><i class="fa fa-check"></i><b>1.11.1</b> Key Concepts</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="introduction-1.html"><a href="introduction-1.html#configuring-generative-ai-models"><i class="fa fa-check"></i><b>1.12</b> Configuring Generative AI Models</a></li>
<li class="chapter" data-level="1.13" data-path="introduction-1.html"><a href="introduction-1.html#generative-ai-project-lifecycle"><i class="fa fa-check"></i><b>1.13</b> Generative AI project lifecycle</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="introduction-1.html"><a href="introduction-1.html#project-life-cycle-stages"><i class="fa fa-check"></i><b>1.13.1</b> Project Life Cycle Stages</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="introduction-1.html"><a href="introduction-1.html#pre-training-large-language-models"><i class="fa fa-check"></i><b>1.14</b> Pre-training large language models</a></li>
<li class="chapter" data-level="1.15" data-path="introduction-1.html"><a href="introduction-1.html#computational-challenges-of-pre-training"><i class="fa fa-check"></i><b>1.15</b> Computational Challenges of pre-training</a>
<ul>
<li class="chapter" data-level="1.15.1" data-path="introduction-1.html"><a href="introduction-1.html#memory-challenges-in-training-llms"><i class="fa fa-check"></i><b>1.15.1</b> Memory Challenges in Training LLMs</a></li>
<li class="chapter" data-level="1.15.2" data-path="introduction-1.html"><a href="introduction-1.html#quantization-techniques"><i class="fa fa-check"></i><b>1.15.2</b> Quantization Techniques</a></li>
<li class="chapter" data-level="1.15.3" data-path="introduction-1.html"><a href="introduction-1.html#distributed-computing"><i class="fa fa-check"></i><b>1.15.3</b> <strong>Distributed Computing</strong></a></li>
<li class="chapter" data-level="1.15.4" data-path="introduction-1.html"><a href="introduction-1.html#practical-use-cases-of-quantization"><i class="fa fa-check"></i><b>1.15.4</b> <strong>Practical Use Cases of Quantization</strong></a></li>
</ul></li>
<li class="chapter" data-level="1.16" data-path="introduction-1.html"><a href="introduction-1.html#scaling-laws"><i class="fa fa-check"></i><b>1.16</b> Scaling Laws</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html"><i class="fa fa-check"></i><b>2</b> Fine tuning LLMs with Instructions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#fine-tuning-on-a-single-task"><i class="fa fa-check"></i><b>2.1</b> Fine-tuning on a single task</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#key-points"><i class="fa fa-check"></i><b>2.1.1</b> Key Points:</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#multi-task-instruction-fine-tuning"><i class="fa fa-check"></i><b>2.2</b> Multi-task, instruction fine-tuning</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#custom-fine-tuning-for-specific-use-cases"><i class="fa fa-check"></i><b>2.2.1</b> Custom Fine-Tuning for Specific Use Cases</a></li>
<li class="chapter" data-level="2.2.2" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#evaluation-and-next-steps"><i class="fa fa-check"></i><b>2.2.2</b> <strong>Evaluation and Next Steps</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#evaluating-model-performance-in-language-tasks"><i class="fa fa-check"></i><b>2.3</b> Evaluating Model Performance in Language Tasks</a></li>
<li class="chapter" data-level="2.4" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#benchmarks"><i class="fa fa-check"></i><b>2.4</b> Benchmarks</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#key-benchmarks-for-llm-evaluation"><i class="fa fa-check"></i><b>2.4.1</b> Key Benchmarks for LLM Evaluation:</a></li>
<li class="chapter" data-level="2.4.2" data-path="fine-tuning-llms-with-instructions.html"><a href="fine-tuning-llms-with-instructions.html#summary"><i class="fa fa-check"></i><b>2.4.2</b> Summary:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="fine-tuning.html"><a href="fine-tuning.html"><i class="fa fa-check"></i><b>3</b> Fine Tuning</a>
<ul>
<li class="chapter" data-level="3.1" data-path="fine-tuning.html"><a href="fine-tuning.html#intro"><i class="fa fa-check"></i><b>3.1</b> Intro</a></li>
<li class="chapter" data-level="3.2" data-path="fine-tuning.html"><a href="fine-tuning.html#fine-tuning-overview"><i class="fa fa-check"></i><b>3.2</b> Fine-Tuning Overview</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="fine-tuning.html"><a href="fine-tuning.html#preparing-training-data"><i class="fa fa-check"></i><b>3.2.1</b> Preparing Training Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="fine-tuning.html"><a href="fine-tuning.html#fine-tuning-process"><i class="fa fa-check"></i><b>3.2.2</b> Fine-Tuning Process</a></li>
<li class="chapter" data-level="3.2.3" data-path="fine-tuning.html"><a href="fine-tuning.html#evaluation-and-results"><i class="fa fa-check"></i><b>3.2.3</b> Evaluation and Results</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="fine-tuning.html"><a href="fine-tuning.html#fine-tuning-of-large-language-models-llms-and-the-associated-challenges"><i class="fa fa-check"></i><b>3.3</b> Fine-tuning of large language models (LLMs) and the associated challenges</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="fine-tuning.html"><a href="fine-tuning.html#fine-tuning-for-specific-tasks"><i class="fa fa-check"></i><b>3.3.1</b> Fine-Tuning for Specific Tasks</a></li>
<li class="chapter" data-level="3.3.2" data-path="fine-tuning.html"><a href="fine-tuning.html#strategies-to-avoid-catastrophic-forgetting"><i class="fa fa-check"></i><b>3.3.2</b> Strategies to Avoid Catastrophic Forgetting</a></li>
<li class="chapter" data-level="3.3.3" data-path="fine-tuning.html"><a href="fine-tuning.html#parameter-efficient-fine-tuning-peft"><i class="fa fa-check"></i><b>3.3.3</b> Parameter Efficient Fine-Tuning (PEFT)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="fine-tuning.html"><a href="fine-tuning.html#multitask-fine-tuning"><i class="fa fa-check"></i><b>3.4</b> Multitask Fine-Tuning</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="fine-tuning.html"><a href="fine-tuning.html#flan-models"><i class="fa fa-check"></i><b>3.4.1</b> FLAN Models</a></li>
<li class="chapter" data-level="3.4.2" data-path="fine-tuning.html"><a href="fine-tuning.html#example-dataset-samsum"><i class="fa fa-check"></i><b>3.4.2</b> Example Dataset: SAMSum</a></li>
<li class="chapter" data-level="3.4.3" data-path="fine-tuning.html"><a href="fine-tuning.html#fine-tuning-for-specific-use-cases"><i class="fa fa-check"></i><b>3.4.3</b> Fine-Tuning for Specific Use Cases</a></li>
<li class="chapter" data-level="3.4.4" data-path="fine-tuning.html"><a href="fine-tuning.html#evaluation-of-model-performance"><i class="fa fa-check"></i><b>3.4.4</b> Evaluation of Model Performance</a></li>
<li class="chapter" data-level="3.4.5" data-path="fine-tuning.html"><a href="fine-tuning.html#what-is-the-purpose-of-fine-tuning-with-prompt-datasets"><i class="fa fa-check"></i><b>3.4.5</b> What is the purpose of fine-tuning with prompt datasets?</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="fine-tuning.html"><a href="fine-tuning.html#evaluating-the-performance-of-large-language-models"><i class="fa fa-check"></i><b>3.5</b> Evaluating the performance of large language models</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="fine-tuning.html"><a href="fine-tuning.html#evaluation-metrics"><i class="fa fa-check"></i><b>3.5.1</b> Evaluation Metrics</a></li>
<li class="chapter" data-level="3.5.2" data-path="fine-tuning.html"><a href="fine-tuning.html#understanding-rouge"><i class="fa fa-check"></i><b>3.5.2</b> Understanding ROUGE</a></li>
<li class="chapter" data-level="3.5.3" data-path="fine-tuning.html"><a href="fine-tuning.html#limitations-and-improvements"><i class="fa fa-check"></i><b>3.5.3</b> Limitations and Improvements</a></li>
<li class="chapter" data-level="3.5.4" data-path="fine-tuning.html"><a href="fine-tuning.html#using-bleu-score"><i class="fa fa-check"></i><b>3.5.4</b> Using BLEU Score</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://davutemrah.github.io" target="blank">Back to Home Page</li>
<li><a href="https://davutemrah.github.io/notebooks/" target="blank">Back to Collections</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Generative AI with Large Language Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fine-tuning" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Fine Tuning<a href="fine-tuning.html#fine-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="intro" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Intro<a href="fine-tuning.html#intro" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Instruction Fine-Tuning</strong></p>
<ul>
<li><p>Instruction fine-tuning adjusts a pre-trained model to better respond to specific prompts and tasks.</p></li>
<li><p>It is a significant advancement, allowing models trained on vast datasets to learn how to follow instructions effectively.</p></li>
</ul>
<p><strong>Challenges in Fine-Tuning</strong></p>
<ul>
<li><p>Catastrophic forgetting can occur, where the model loses previously learned information during fine-tuning.</p></li>
<li><p>Techniques to mitigate this include using a diverse range of instruction types during the fine-tuning process.</p></li>
</ul>
<p><strong>Parameter Efficient Fine-Tuning (PEFT)</strong></p>
<ul>
<li><p>PEFT methods allow for fine-tuning without adjusting all model parameters, reducing computational and memory costs.</p></li>
<li><p>Techniques like LoRA (Low-Rank Adaptation) enable effective fine-tuning with minimal resource requirements, making it accessible for various applications.</p></li>
</ul>
</div>
<div id="fine-tuning-overview" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Fine-Tuning Overview<a href="fine-tuning.html#fine-tuning-overview" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Fine-tuning is a supervised learning process that updates the weights of a base model using a dataset of labeled examples.</p></li>
<li><p>Instruction fine-tuning is a method that improves a model’s performance across various tasks by training it with examples that demonstrate how to respond to specific instructions.</p></li>
</ul>
<div id="preparing-training-data" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Preparing Training Data<a href="fine-tuning.html#preparing-training-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>To fine-tune a model, you need to prepare a dataset of prompt-completion pairs, which includes instructions relevant to the task.</p></li>
<li><p>Prompt template libraries can help convert existing datasets into instruction prompt datasets suitable for fine-tuning.</p></li>
</ul>
</div>
<div id="fine-tuning-process" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Fine-Tuning Process<a href="fine-tuning.html#fine-tuning-process" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The fine-tuning process involves selecting prompts from the training dataset, generating completions with the LLM, and comparing these completions to the expected responses.</p></li>
<li><p>The model’s weights are updated based on the calculated loss between the generated output and the training labels, improving its performance over multiple iterations.</p></li>
</ul>
</div>
<div id="evaluation-and-results" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Evaluation and Results<a href="fine-tuning.html#evaluation-and-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>After fine-tuning, the model’s performance is evaluated using validation and test datasets to measure accuracy.</p></li>
<li><p>The outcome is a new version of the model, often referred to as an instruct model, which is better suited for the specific tasks you are interested in.</p></li>
</ul>
</div>
</div>
<div id="fine-tuning-of-large-language-models-llms-and-the-associated-challenges" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Fine-tuning of large language models (LLMs) and the associated challenges<a href="fine-tuning.html#fine-tuning-of-large-language-models-llms-and-the-associated-challenges" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="fine-tuning-for-specific-tasks" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Fine-Tuning for Specific Tasks<a href="fine-tuning.html#fine-tuning-for-specific-tasks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Fine-tuning a pre-trained model can significantly improve its performance on a specific task, such as summarization, using a limited dataset of 500-1,000 examples.</p></li>
<li><p>A downside is catastrophic forgetting, where the model loses its ability to perform other tasks it was trained on.</p></li>
</ul>
</div>
<div id="strategies-to-avoid-catastrophic-forgetting" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Strategies to Avoid Catastrophic Forgetting<a href="fine-tuning.html#strategies-to-avoid-catastrophic-forgetting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Determine if catastrophic forgetting impacts your application; if only one task is needed, it may not be an issue.</p></li>
<li><p>For maintaining multitask capabilities, consider fine-tuning on multiple tasks at once, which requires a larger dataset of 50-100,000 examples.</p></li>
</ul>
</div>
<div id="parameter-efficient-fine-tuning-peft" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Parameter Efficient Fine-Tuning (PEFT)<a href="fine-tuning.html#parameter-efficient-fine-tuning-peft" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>PEFT techniques focus on preserving the original model’s weights while training a small number of task-specific parameters, which helps mitigate catastrophic forgetting.</p></li>
<li><p>This area is actively researched and will be discussed in more detail later in the course.</p></li>
</ul>
</div>
</div>
<div id="multitask-fine-tuning" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Multitask Fine-Tuning<a href="fine-tuning.html#multitask-fine-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>It involves training a model on a dataset that includes multiple tasks, such as summarization, review rating, code translation, and entity recognition.</p></li>
<li><p>This approach helps improve the model’s performance across all tasks simultaneously and mitigates the issue of catastrophic forgetting.</p></li>
</ul>
<div id="flan-models" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> FLAN Models<a href="fine-tuning.html#flan-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The FLAN family of models, which stands for fine-tuned language net, is an example of models trained using multitask instruction fine-tuning.</p></li>
<li><p>FLAN-T5 and FLAN-PALM are specific instruct versions of the T5 and PALM foundation models, respectively, fine-tuned on numerous datasets.</p></li>
</ul>
</div>
<div id="example-dataset-samsum" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Example Dataset: SAMSum<a href="fine-tuning.html#example-dataset-samsum" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>SAMSum is a dataset used for training models to summarize dialogues, containing 16,000 conversations with human-crafted summaries.</p></li>
<li><p>The dataset is designed to reflect real-life messenger conversations, ensuring high-quality training data for language models.</p></li>
</ul>
</div>
<div id="fine-tuning-for-specific-use-cases" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Fine-Tuning for Specific Use Cases<a href="fine-tuning.html#fine-tuning-for-specific-use-cases" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Additional fine-tuning can be performed on models like FLAN-T5 using domain-specific datasets, such as dialogsum, to improve performance in particular contexts, like customer service chats.</p></li>
<li><p>The importance of using internal company data for fine-tuning is emphasized to tailor the model to specific summarization needs.</p></li>
</ul>
</div>
<div id="evaluation-of-model-performance" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Evaluation of Model Performance<a href="fine-tuning.html#evaluation-of-model-performance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>The content highlights the need to evaluate the quality of model outputs after fine-tuning, which will be discussed in the next video.</li>
</ul>
</div>
<div id="what-is-the-purpose-of-fine-tuning-with-prompt-datasets" class="section level3 hasAnchor" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> What is the purpose of fine-tuning with prompt datasets?<a href="fine-tuning.html#what-is-the-purpose-of-fine-tuning-with-prompt-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To improve the performance and adaptability of a pre-trained language model for specific tasks. This option accurately describes the purpose of fine-tuning with prompt datasets. It aims to improve the performance and adaptability of a pre-trained language model by training it on specific tasks using instruction prompts.</p>
</div>
</div>
<div id="evaluating-the-performance-of-large-language-models" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Evaluating the performance of large language models<a href="fine-tuning.html#evaluating-the-performance-of-large-language-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since, LLM are probabilictic and there are millions of results to evaluate, human evaluation is not possible and requires a systematic solution.</p>
<div id="evaluation-metrics" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Evaluation Metrics<a href="fine-tuning.html#evaluation-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is used to assess the quality of automatically generated summaries by comparing them to human-generated references.</p></li>
<li><p>BLEU (Bilingual Evaluation Understudy) evaluates the quality of machine-translated text by comparing n-grams in the generated translation to those in a reference translation.</p></li>
</ul>
</div>
<div id="understanding-rouge" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Understanding ROUGE<a href="fine-tuning.html#understanding-rouge" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>ROUGE-1 measures unigram matches (single words) between generated and reference sentences, calculating recall, precision, and F1 scores.</p></li>
<li><p>ROUGE-2 extends this by considering bigrams (pairs of words), which helps account for word order.</p></li>
</ul>
</div>
<div id="limitations-and-improvements" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Limitations and Improvements<a href="fine-tuning.html#limitations-and-improvements" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>Simple ROUGE scores can be misleading, as they may yield high scores for poor outputs. Clipping functions can be used to limit unigram matches to their maximum count in the reference.</p></li>
<li><p>The ROUGE-L score uses the longest common subsequence to provide a more nuanced evaluation.</p></li>
</ul>
</div>
<div id="using-bleu-score" class="section level3 hasAnchor" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Using BLEU Score<a href="fine-tuning.html#using-bleu-score" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>The BLEU score averages precision across multiple n-gram sizes, providing a comprehensive measure of translation quality.</p></li>
<li><p>Both ROUGE and BLEU are low-cost metrics useful for iterative model evaluation, but should not be the sole indicators of model performance.</p></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fine-tuning-llms-with-instructions.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Generative AI.pdf", "Generative AI.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
