[["index.html", "Generative AI with Large Language Models Introduction", " Generative AI with Large Language Models DEA 2025-12-06 Introduction "],["introduction-1.html", "Chapter 1 Introduction 1.1 Level 1: GenAI &amp; LLMs Basics 1.2 Level 2: AI Agent Development 1.3 Level 3: AI Agents in Production + Multi-Agents 1.4 Generative AI Overview 1.5 Use Cases of LLM 1.6 Text Generation before Transformers 1.7 Introduction of Transformer Architecture 1.8 Transformers 1.9 Transformer Architecture 1.10 Some well-known models 1.11 Prompt Engineering 1.12 Configuring Generative AI Models 1.13 Generative AI project lifecycle 1.14 Pre-training large language models 1.15 Computational Challenges of pre-training 1.16 Scaling Laws", " Chapter 1 Introduction These 6 repositories, 5 courses, and 4 books on AI Agents are all you need to go from 0 ‚Üí Production Level. These resources are the best I‚Äôve seen so far. üëá 1.1 Level 1: GenAI &amp; LLMs Basics Generative AI for Everyone ‚Äî Course Large Language Models (LLMs) ‚Äì Level 1 ‚Äî Course Hands-On LLMs ‚Äî Book LLM Course ‚Äî Repo Prompt Engineering Guide ‚Äî Repo 1.2 Level 2: AI Agent Development Introduction to RAG ‚Äî Course RAG Techniques ‚Äî Repo GenAI Agents ‚Äî Repo AI Agents in Practice ‚Äî Book Agentic Design Patterns ‚Äî Book 1.3 Level 3: AI Agents in Production + Multi-Agents Agents Towards Production ‚Äî Repo LLMs in Enterprise ‚Äî Book Multi-AI Agent Systems with crewAI ‚Äî Course 1.4 Generative AI Overview Generative AI tools have become widely accessible and are capable of creating content that mimics or approximates human ability. Examples include: Chatbots Image generation from text Code development plugins 1.4.1 What is Generative AI? Subset of Traditional Machine Learning: Generative AI models learn by finding statistical patterns in massive datasets originally created by humans. Training: Large language models (LLMs) are trained on trillions of words over many weeks or months using significant computational power. Foundation Models: These models, sometimes called base models, have billions of parameters and exhibit emergent properties beyond language, including reasoning and problem-solving abilities. Bert, GPT, LLaMa, BLOOM, PaLM, FLAN-T5 1.4.2 Understanding Parameters Parameters as Memory: Think of parameters as the model‚Äôs memory. More parameters mean more memory, which typically leads to the ability to perform more sophisticated tasks. Model Representation: Throughout the course, LLMs will be represented by purple circles. Example Model: In the labs, you will use an open-source model called flan-T5 for language tasks. 1.4.3 Model Adaptation and Use Fine Tuning: You can either use these models as they are or apply fine-tuning techniques to adapt them to specific use cases without the need to train a new model from scratch. Multimodal Generative AI: While generative AI models exist for images, video, audio, and speech, this course focuses on LLMs and natural language generation. 1.4.4 Interaction with LLMs Natural Language Prompts: Unlike traditional machine learning and programming paradigms that use formalized syntax, LLMs take natural language instructions (prompts) and perform tasks. Context Window: The space available for a prompt is called the context window, typically large enough for a few thousand words. Prompt ‚Äì&gt; LLM ‚Äì&gt; Completion (generated text) 1.5 Use Cases of LLM The broader applications of large language models (LLMs) and generative AI beyond chatbots. It highlights that while chatbots are prominent, LLMs can also perform diverse tasks, such as: Text Generation: LLMs can generate essays based on prompts and summarize dialogues. Translation: They can translate between different languages and convert natural language into machine code, such as generating Python code. Information Retrieval: LLMs can identify named entities (people and places) in texts through tasks like named entity recognition. Augmentation: There‚Äôs ongoing development in connecting LLMs to external data sources and APIs to enhance their capabilities and access real-time information. Larger models (with billions of parameters) show improved language understanding, although smaller models can be fine-tuned for specific tasks. The rapid advancements in LLM capabilities are attributed to their underlying architecture. 1.6 Text Generation before Transformers Introduction to Generative Algorithms Generative algorithms have been a part of natural language processing for a long time. Before the advent of transformer models, the primary architecture used for generative tasks was recurrent neural networks (RNNs). 1.6.1 Recurrent Neural Networks (RNNs) Architecture: RNNs process sequences of data by maintaining a hidden state that captures information from previous time steps. Limitations: Computational and Memory Constraints: RNNs require significant computational resources and memory to process long sequences. Short-Term Memory: RNNs struggle with long-term dependencies due to vanishing gradients, leading to poor performance on tasks requiring an understanding of extended context. 1.6.2 Example of RNN in Action Simple Next-Word Prediction Task: With only one preceding word, the RNN‚Äôs prediction is not very accurate. Scaling the RNN to consider more preceding words increases computational complexity and resource usage. Despite scaling, the model often fails to capture enough context for accurate predictions. 1.6.3 Challenges in Language Understanding Context Requirement: Successful next-word prediction requires understanding the entire sentence or document. Complexity of Language: Homonyms: Words with multiple meanings depending on context (e.g., ‚Äúbank‚Äù as a financial institution or the side of a river). Syntactic Ambiguity: Sentence structures can be ambiguous (e.g., ‚ÄúThe teacher taught the students with the book‚Äù ‚Äì did the teacher use the book or did the students have the book?). 1.7 Introduction of Transformer Architecture In 2017, the transformer architecture revolutionized generative AI with the publication of the paper ‚ÄúAttention is All You Need‚Äù by researchers from Google and the University of Toronto. 1.7.1 Advantages of Transformers Efficient Scaling: Can be efficiently scaled to use multi-core GPUs. Parallel Processing: Processes input data in parallel, allowing for the use of larger training datasets. Attention Mechanism: Learns to pay attention to the meaning of words in context. Addresses the limitations of RNNs by considering the relevance of each word in the input sequence to every other word. The transformer architecture marked a significant breakthrough in natural language processing, enabling models to handle complex generative tasks more efficiently and accurately. The title of the influential paper, ‚ÄúAttention is All You Need,‚Äù underscores the importance of the attention mechanism in transforming the field of generative AI. 1.8 Transformers 1.8.1 Overview The transformer architecture significantly improved natural language processing tasks compared to earlier RNNs, enabling superior generative capabilities. Its power lies in learning the relevance and context of words across a sentence using attention mechanisms. 1.8.2 Attention Mechanisms Self-Attention: This mechanism learns the relationships between all words in a sentence, allowing the model to understand the context and relevance of each word in relation to others. Attention Map: A visual representation of the attention weights, showing how words relate to each other. For example, the word ‚Äúbook‚Äù might strongly connect with ‚Äúteacher‚Äù and ‚Äústudent‚Äù. 1.8.3 Transformer Architecture The transformer model consists of two main components: Encoder: Encodes input sequences into deep representations. Decoder: Uses these representations to generate output sequences. 1.8.4 Tokenization Converts words into numbers representing their positions in a dictionary. Tokenization methods can vary, representing whole words or parts of words. Consistent tokenization is essential for both training and generating text. 1.8.5 Embedding Layer Transforms token IDs into high-dimensional vectors. Encodes the meaning and context of tokens in a vector space. 1.8.6 Positional Encoding Adds information about the position of words in a sentence, preserving word order relevance. 1.8.7 Self-Attention Layer Analyzes relationships between tokens in the input sequence. Multi-headed self-attention means multiple sets of self-attention weights are learned in parallel. Each attention head learns different aspects of language, enhancing contextual understanding. 1.8.8 Feed-Forward Network Processes outputs from the self-attention layer. Produces logits proportional to the probability of each token in the dictionary. 1.8.9 Softmax Layer Normalizes logits into probability scores for each token. The highest probability token is selected as the next word in the sequence. 1.8.10 Prediction Process Let‚Äôs walk through a sequence-to-sequence task, such as translating a French phrase into English: Tokenize Input: The French phrase is tokenized using the same tokenizer that trained the network. Encoder: Tokenized input is passed through the embedding layer and multi-headed attention layers, producing a deep representation of the input sequence. Decoder: A start-of-sequence token triggers the decoder to predict the next token. The decoder uses the encoder‚Äôs contextual understanding to generate the output token. Loop: The output token is fed back into the decoder to predict the next token until an end-of-sequence token is predicted. Detokenization: The sequence of tokens is converted back into words to form the final output. 1.8.11 Example Input: French phrase ‚ÄúJe t‚Äôaime la machine d‚Äôapprentissage‚Äù. Tokenized input is processed through the encoder and decoder. Output: ‚ÄúI love machine learning‚Äù. 1.9 Transformer Architecture 1.9.1 Overview The transformer architecture consists of encoder and decoder components, essential for various natural language processing tasks. 1.9.2 Encoder Function: Encodes input sequences into a deep representation of their structure and meaning. Usage: You can train encoder-only models to perform classification tasks such as sentiment analysis Encoder-only models, such as BERT, work as sequence-to-sequence models with equal input and output sequence lengths. With additional layers, you can train encoder-only models to perform classification tasks like sentiment analysis. 1.9.3 Decoder Function: Uses the encoder‚Äôs contextual understanding to generate new tokens, operating in a loop until a stop condition is met. Usage: Decoder-only models, such as the GPT family, BLOOM, Jurassic, and LLaMA, are commonly used today and can generalize to most tasks. 1.9.4 Encoder-Decoder Models Function: Handle sequence-to-sequence tasks where input and output sequences can differ in length. Examples: BART, T5. Usage: Suitable for tasks like translation and general text generation. 1.9.5 Practical Application Main Goal: Understand the differences between various models to read model documentation effectively. Prompt Engineering: Interact with transformer models through natural language prompts, focusing on written words rather than code. Transformers: Attention is all you need ‚ÄúAttention is All You Need‚Äù is a research paper published in 2017 by Google researchers, which introduced the Transformer model, a novel architecture that revolutionized the field of natural language processing (NLP) and became the basis for the LLMs we now know - such as GPT, PaLM and others. The paper proposes a neural network architecture that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with an entirely attention-based mechanism. The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively. The authors demonstrate that their model achieves state-of-the-art performance on several machine translation tasks and outperforms previous models that rely on RNNs or CNNs. The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network applies a point-wise fully connected layer to each position separately and identically. The Transformer model also uses residual connections and layer normalization to facilitate training and prevent overfitting. In addition, the authors introduce a positional encoding scheme that encodes the position of each token in the input sequence, enabling the model to capture the order of the sequence without the need for recurrent or convolutional operations. 1.10 Some well-known models 1.10.1 BERT BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model developed by Google in 2018, designed specifically for natural language understanding (NLU) tasks. BERT differs from earlier models because of its bidirectional training approach, which allows it to understand context in a nuanced way by considering both the left and right context in a sentence. 1.10.1.1 Key Details About BERT: Parameter Sizes: BERT Base: 110 million parameters (12 layers, 768 hidden units, 12 attention heads) BERT Large: 340 million parameters (24 layers, 1024 hidden units, 16 attention heads) Bidirectional Training: BERT‚Äôs bidirectional approach uses masked language modeling (MLM) during training. This means it randomly masks words in a sentence and trains the model to predict them based on the surrounding context, giving it a deeper understanding of word relationships. Pretraining Tasks: Masked Language Modeling (MLM): BERT masks a portion of the words in a sentence and tries to predict the missing words. Next Sentence Prediction (NSP): BERT also learns to predict if one sentence logically follows another, which helps with tasks like question answering and sentence-pair classification. Fine-Tuning for Specific Tasks: BERT‚Äôs architecture is flexible, allowing it to be fine-tuned for a range of NLP tasks with minimal additional training. Common tasks include: Sentiment Analysis Named Entity Recognition (NER) Question Answering (e.g., SQuAD benchmark) Text Classification and Similarity Tasks Impact and Variants: BERT set a new benchmark in NLP, and its architecture has inspired many derivatives and improvements, including RoBERTa (by Facebook AI), DistilBERT (a smaller, faster BERT), and ALBERT (a more efficient BERT variant). BERT transformed NLP by demonstrating how powerful bidirectional transformers can be for understanding language context, which has since influenced other models like GPT, T5, and BLOOM. 1.10.2 BLOOM BLOOM (BigScience Large Open-science Open-access Multilingual Language Model) is a large language model created by the BigScience project, an open scientific collaboration led by Hugging Face with researchers worldwide. BLOOM is designed to handle multiple languages and tasks, such as text generation, summarization, and translation, in a way similar to models like GPT-3. 1.10.2.1 Key Details About BLOOM: Parameter Size: BLOOM has 176 billion parameters in its largest version, making it one of the largest open-access language models available. There are smaller versions with fewer parameters, which are used for different applications requiring less computational power. Multilingual Capacity: BLOOM was trained on 46 languages and 13 programming languages, including English, French, Arabic, Spanish, Chinese, and more, making it versatile in multilingual NLP tasks. Training Data: The model was trained on a vast, diverse dataset of over 1.5 terabytes of text data, which includes a variety of domains to help BLOOM perform across multiple contexts and languages. Open Access: One of the primary goals of the BLOOM model is transparency and open accessibility for the AI research community, unlike other large models that are proprietary. If you‚Äôre working in NLP or language modeling, BLOOM is an excellent model to explore due to its openness, multilingual capabilities, and state-of-the-art performance across various tasks. 1.11 Prompt Engineering Introduction Prompt engineering involves crafting and refining the input text (prompt) fed to a model to influence its behavior during inference, resulting in desired output (completion). The total text available for the prompt is called the context window. 1.11.1 Key Concepts Prompt Engineering Definition: The process of developing and improving the prompt to achieve desired model behavior. Strategy: Including examples of the task within the prompt, known as in-context learning. In-Context Learning Definition: Helping models learn the task by including examples in the prompt. Example: For sentiment analysis, the prompt can include the instruction, review text, and an expected sentiment output. Zero-Shot Inference Definition: Providing only the input data in the prompt without examples. Effectiveness: Large models perform well; smaller models may struggle. One-Shot Inference Definition: Including a single example in the prompt to guide the model. Example: A sample review and sentiment analysis followed by the actual input review. Few-Shot Inference Definition: Including multiple examples in the prompt to improve model understanding. Example: A mix of positive and negative reviews to guide sentiment analysis. Practical Considerations Context Window Limitation: There‚Äôs a limit on the amount of in-context learning that can be included. Recommendation: If performance doesn‚Äôt improve with multiple examples, consider fine-tuning the model. Fine-Tuning Definition: Additional training on the model with new data to improve task-specific performance. Upcoming: Detailed exploration of fine-tuning will be covered in week 2 of the course. Model Performance and Scale Observation: Model performance on various tasks depends on the scale (number of parameters). Large Models: Good at zero-shot inference for multiple tasks. Smaller Models: Generally limited to tasks similar to their training data. Experimentation Recommendation: Experiment with different models and settings to find the best fit for your use case. Next Steps: Explore configuration settings to influence the structure and style of model completions. 1.12 Configuring Generative AI Models Overview This lecture examines methods and configuration parameters used to influence a model‚Äôs next-word generation during inference. Unlike training parameters, these settings are adjusted at inference time to control aspects like the maximum number of tokens generated and the creativity of the output. Key Configuration Parameters 1. Max New Tokens Definition: Limits the number of tokens the model generates. Usage: Set to values like 100, 150, or 200 to cap the generation process. Example: If set to 200, the generation might end sooner if an end-of-sequence token is predicted. 2. Greedy Decoding Definition: The model always selects the word with the highest probability. Characteristics: Works well for short texts but can lead to repetitive outputs. 3. Random Sampling Definition: Selects the next word based on the probability distribution. Advantages: Introduces variability to avoid repetitive text. Disadvantages: Can produce outputs that are too creative or nonsensical. Implementation: In Hugging Face, set do_sample=True. 4. Top-k Sampling Definition: Limits choices to the top k highest probability tokens. Example: If k=3, the model selects from the top 3 probable words. Benefit: Balances randomness and coherence in the output. 5. Top-p (Nucleus) Sampling Definition: Chooses from tokens whose cumulative probability meets a threshold p. Example: If p=0.3, selects from tokens that together have a probability of 0.3. Benefit: Ensures sensible and coherent generation by limiting low-probability words. 6. Temperature Definition: Controls the randomness by scaling the probability distribution. Effect: Low temperature (&lt;1): Concentrates probability on fewer words, producing less random and more predictable text. High temperature (&gt;1): Spreads probability more evenly, increasing randomness and creativity. Temperature=1: Uses the default probability distribution. 1.13 Generative AI project lifecycle Introduction Throughout this course, you‚Äôll learn the techniques required to develop and deploy an LLM-powered application. This video introduces the generative AI project life cycle, guiding you from conception to launch. 1.13.1 Project Life Cycle Stages 1.13.1.1 Define the Scope Importance: Accurately and narrowly define the project‚Äôs scope. Considerations: What specific function will the LLM serve in your application? Does the model need to perform various tasks or focus on one specific task (e.g., named entity recognition)? Outcome: Save time and compute costs by clearly defining requirements. 1.13.1.2 Choose a Model Decision: Train your own model from scratch or use an existing base model. Common Approach: Start with an existing model. Considerations: Later in the course, you‚Äôll learn rules of thumb to help estimate the feasibility of training your own model. 1.13.1.3 Assess and Train the Model Initial Steps: Use prompt engineering and in-context learning to improve model performance. Fine-Tuning: Necessary if prompt engineering is insufficient. Supervised learning process covered in Week 2. Reinforcement Learning with Human Feedback (RLHF): Ensures the model behaves well and aligns with human preferences. - Covered in Week 3. 1.13.1.4 Evaluate the Model Metrics and Benchmarks: Explore next week. Iterative Process: Start with prompt engineering. Evaluate outputs and fine-tune if necessary. Revisit and refine prompt engineering. 1.13.1.5 Deployment Final Steps: Optimize the model for deployment. Integrate the model with your application. Ensure efficient use of compute resources for a better user experience. 1.13.1.6 Additional Infrastructure Considerations Limitations of LLMs: Tendency to invent information when unsure. Limited ability to perform complex reasoning and mathematics. Overcoming Limitations: Learn techniques to address these issues in the final part of the course. 1.13.1.7 Conclusion This framework maps out the tasks required to take your project from conception to launch, providing insights into important decisions, potential difficulties, and necessary infrastructure for developing and deploying your application. 1.14 Pre-training large language models Generative AI Project Life Cycle Overview The process begins with scoping the use case and determining how an LLM will integrate into the application. Next, choose a model: either use an existing model or train one from scratch (the latter is explored in detail later). Choosing a Model Start with existing foundation models available on hubs like Hugging Face or PyTorch, which include model cards detailing use cases, training processes, and limitations. The model choice depends on the task and the architecture‚Äôs strengths. Transformer Model Variants Encoder-Only Models (Autoencoding Models): Pre-trained with masked language modeling to reconstruct input by predicting masked tokens. Suited for sentence classification (e.g., sentiment analysis) and token classification (e.g., named entity recognition). Examples: BERT, RoBERTa. Decoder-Only Models (Autoregressive Models): Pre-trained with causal language modeling to predict the next token. Best for text generation and capable of zero-shot inference. Examples: GPT, BLOOM. Sequence-to-Sequence Models: Utilize both encoder and decoder components. Pre-trained with tasks like span corruption (e.g., T5) or other methods. Ideal for translation, summarization, and question answering. Examples: T5, BART. Pre-Training Process LLMs are trained on large datasets using self-supervised learning to capture language patterns. Data preprocessing ensures quality and reduces bias, with only 1‚Äì3% of tokens retained for training. Large-scale compute resources and GPUs are required. Model Size and Performance Larger models exhibit better performance with reduced need for in-context learning or further training. Advances in transformers, data availability, and compute power have driven the creation of increasingly larger models. However, training large models is expensive and resource-intensive, raising challenges in scaling. 1.15 Computational Challenges of pre-training This detailed explanation of memory issues in large language models (LLMs) and the solutions available highlights several key points about quantization, precision, and distributed computing in the context of training large models. Below is a structured summary of the information: Quantization is essential for addressing memory constraints in training and fine-tuning LLMs. BFLOAT16 offers an optimal trade-off between precision and memory savings and is increasingly used in modern GPUs. Distributed computing enables scaling beyond GPU memory limits but incurs high costs and complexity. 1.15.1 Memory Challenges in Training LLMs GPU Memory Limitations: Storing model weights and additional overheads during training (e.g., gradients, optimizers) requires significant GPU memory. Example: A 1 billion parameter model needs ~24 GB of GPU RAM to train in 32-bit (FP32) precision. Exponential Memory Requirements: Large-scale models, often exceeding 50‚Äì100 billion parameters, demand tens of thousands of gigabytes of GPU memory, far exceeding single-GPU or consumer hardware capabilities. 1.15.2 Quantization Techniques Concept: Reduces the precision of floating-point numbers representing model weights and activations, minimizing memory usage while maintaining acceptable performance. Popular Data Types: FP32 (Full Precision): 32 bits, highest precision; requires 4 bytes. FP16 (Half Precision): 16 bits, uses 2 bytes; sacrifices precision. BFLOAT16 (Brain Floating Point): A truncated 32-bit float using 16 bits, developed by Google Brain, offering a balance of precision and memory efficiency. INT8: 8-bit integers; extremely memory-efficient but with significant loss in precision. Comparison of Precision: FP32: Range \\(-3 \\times 10^{38}\\) to \\(3 \\times 10^{38}\\). FP16: Range \\(-65,504\\) to \\(65,504\\). BFLOAT16: Maintains the full dynamic range of FP32 but with only 7 bits for precision. INT8: Range \\(-128\\) to \\(127\\); projects values like \\(\\pi\\) to coarse approximations (e.g., 3). Memory Savings: FP16: Reduces memory by 50% compared to FP32. INT8: Reduces memory by 75%. 1.15.3 Distributed Computing Scaling Beyond Single GPUs: As model sizes grow, training on a single GPU becomes impractical. Distributed computing across hundreds of GPUs is required for training models with tens or hundreds of billions of parameters. Distributed setups are cost-prohibitive for many researchers, contributing to the dominance of pre-trained and fine-tuned models. 1.15.4 Practical Use Cases of Quantization Fine-Tuning: Even during fine-tuning, storing all model parameters in memory is necessary, making memory efficiency critical. Popular models like FLAN-T5 are pre-trained with BFLOAT16, showcasing its widespread adoption. Quantization-Aware Training (QAT): Modern frameworks support QAT, enabling scaling factors to be learned during training for optimal lower-precision projections. 1.16 Scaling Laws This content discusses research and insights into optimizing large language models by exploring the relationships between model size, training dataset size, and compute budget. Here are the key points: Pre-training Goals: The goal of pre-training language models is to maximize performance by minimizing loss when predicting tokens. Improvements can be achieved by: Increasing the training dataset size. Increasing the number of model parameters. Compute Budget Constraints: Compute budgets, including hardware and time resources, are a practical constraint. A standard unit for compute resources is the petaFLOP per second day, which represents one quadrillion floating-point operations per second for one day. Scaling Trade-Offs: Larger models and datasets generally require more compute resources to train effectively. There is a power-law relationship between compute budget, training dataset size, model size, and performance. Research Insights: OpenAI‚Äôs research demonstrated how compute budget and training data size influence model performance, identifying clear trade-offs. With fixed compute budgets, optimizing dataset size and model parameters is key to improving performance. Chinchilla Paper Findings: Many large language models (e.g., GPT-3) may be over-parameterized (too many parameters) and under-trained (not enough data). Optimal dataset size should be about 20 times the number of model parameters. Models like Chinchilla, trained optimally, outperform larger, non-optimal models such as GPT-3. Trends and Industry Implications: The Chinchilla study indicates that focusing on optimal training (not just larger models) can yield better results. Models like Bloomberg GPT show that compute-efficient training can achieve excellent task-specific performance with smaller parameter sizes. The discussion points towards a shift away from the ‚Äúbigger is better‚Äù trend to a focus on compute-efficient and optimally trained models. "],["fine-tuning-llms-with-instructions.html", "Chapter 2 Fine tuning LLMs with Instructions 2.1 Fine-tuning on a single task 2.2 Multi-task, instruction fine-tuning 2.3 Evaluating Model Performance in Language Tasks 2.4 Benchmarks", " Chapter 2 Fine tuning LLMs with Instructions Prompt Engineering and Inference: Zero-shot: Models respond to prompts without examples. One/Few-shot: Including example outputs improves performance, but it uses context window space and may not work for smaller models. Drawbacks: - In context learning may not work for smaller LLM models - Examples take up valuable space in the context window Fine-tuning Basics: Unlike pre-training on vast data using self-supervised learning, fine-tuning uses labeled examples (prompt-completion pairs) via supervised learning to optimize model weights for specific tasks. Instruction fine-tuning trains models with explicit instruction-based examples for tasks (e.g., ‚ÄúSummarize the following text‚Äù). Data sets of task-specific examples consists of PROMPT + COMPLETION couples Full fine-tuning updates all parameters. Instruction Fine-Tuning Process: Prepare labeled data with templates (e.g., using Amazon reviews to create classification or summarization prompts). There are prompt template libraries turn existing unstructured datasets into instruction prompt datasets for fine tuning Split data into training, validation, and test sets. Train the model using cross-entropy loss and backpropagation. Evaluate using validation and test datasets for accuracy. Outcome: The result is an updated model (instruct model) fine-tuned for your desired tasks. This approach is highly common in LLM optimization and essential for building task-specific solutions. 2.1 Fine-tuning on a single task While LLMs can handle multiple tasks, fine-tuning them for a single task can significantly improve performance, often with just 500‚Äì1,000 examples. However, this may result in catastrophic forgetting‚Äîthe model improves at the fine-tuned task but loses its ability to perform other tasks it previously handled. 2.1.1 Key Points: Single-task Fine-tuning: Focuses on one task (e.g., summarization or sentiment analysis) using task-specific examples. Effective for applications where only one task is required. Catastrophic Forgetting: Occurs because full fine-tuning modifies the original model‚Äôs weights. Catastrophic forgetting occurs when a machine learning model forgets previously learned information as it learns new information. Catastrophic forgetting is a common problem in machine learning, especially in deep learning models. One way to mitigate catastrophic forgetting is by using regularization techniques to limit the amount of change that can be made to the weights of the model during training. Example: A model fine-tuned for sentiment analysis may lose its ability to do named entity recognition. Options to Mitigate Catastrophic Forgetting: Evaluate use case needs: If only one task is needed, catastrophic forgetting may not be an issue. Multitask Fine-tuning: Train the model on multiple tasks using 50,000‚Äì100,000 examples to retain generalized capabilities. Requires more data and compute. Parameter Efficient Fine-tuning (PEFT): Modify only a small number of task-specific layers while preserving most of the original LLM weights. PEFT helps maintain robustness to catastrophic forgetting. These approaches enable tailoring LLMs while balancing task-specific optimization with multitask generalization. 2.2 Multi-task, instruction fine-tuning Multitask fine-tuning trains a model on examples for multiple tasks (e.g., summarization, review rating, entity recognition). Avoids catastrophic forgetting by improving performance across all tasks simultaneously. Requirements: Needs 50,000‚Äì100,000 examples of high-quality data across tasks. Produces a general-purpose model capable of handling diverse tasks well. Example: FLAN Models (Fine-Tuned Language Net): Family of models like FLAN-T5 and FLAN-PaLM are fine-tuned using 473 datasets across 146 tasks. SAMSum dataset (dialogue summaries) is one of the datasets used for fine-tuning FLAN-T5. 2.2.1 Custom Fine-Tuning for Specific Use Cases Scenario: Example: Customer service teams using summaries of support chat conversations. FLAN-T5 may struggle with domain-specific tasks (e.g., customer support chats) not covered in its training. Solution: Use a domain-specific dataset, such as DialogSum, to fine-tune further. DialogSum includes 13,000 support chat dialogues with summaries and helps FLAN-T5 better summarize customer service chats. Benefits of Domain-Specific Fine-Tuning: Improves task performance on unique datasets (e.g., internal customer support chats). Reduces fabricated information and adds accuracy, reflecting the specific context. 2.2.2 Evaluation and Next Steps Evaluate the quality of fine-tuned model outputs using metrics and benchmarks. Internal, domain-specific datasets (e.g., customer service transcripts) can provide even better results tailored to the business needs. 2.3 Evaluating Model Performance in Language Tasks When assessing language model performance, statements like ‚Äúthe model showed improvement‚Äù need formalization through evaluation metrics. While traditional machine learning metrics (e.g., accuracy) are straightforward, evaluating language-based models involves additional challenges, particularly when outputs are non-deterministic. Key Metrics: ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Used for summarization tasks. Measures overlap between model-generated and reference text using: ROUGE-1: Focus on unigram (single word) matches. ROUGE-2: Incorporates bigram (two-word sequence) matches to account for word order. ROUGE-L: Uses the longest common subsequence for recall, precision, and F1-score calculations. Challenges include susceptibility to artificially high scores from repeated words or incorrect word orders. Solutions include unigram clipping and experimenting with n-gram sizes based on sentence structure and use cases. BLEU (Bilingual Evaluation Understudy): Designed for machine translation tasks. Calculates precision over multiple n-gram sizes and averages them. Example: A BLEU score of 0.495 reflects a moderate match, improving as generated text resembles the reference more closely. Best Practices: ROUGE is ideal for diagnosing summarization performance. BLEU is suited for translation evaluation. Both are simple and computationally inexpensive but should not be the sole evaluation tools for large language models. Use standardized benchmarks for comprehensive assessment. Next steps often include benchmarking models against established datasets for a holistic evaluation. Many libraries (e.g., Hugging Face) provide pre-built implementations of these metrics for ease of use. 2.4 Benchmarks Large language models (LLMs) are complex, and traditional metrics like ROUGE and BLEU provide limited insights into their capabilities. To evaluate LLMs more holistically, researchers rely on specialized datasets and benchmarks, which assess various skills, risks, and limitations. Selecting appropriate datasets for evaluation is crucial for understanding an LLM‚Äôs performance, particularly on unseen data. 2.4.1 Key Benchmarks for LLM Evaluation: GLUE (General Language Understanding Evaluation): Introduced in 2018. Focuses on generalization across tasks like sentiment analysis and question-answering. Encourages the development of versatile models. SuperGLUE: Introduced in 2019 to address GLUE‚Äôs limitations. Adds tasks like multi-sentence reasoning and reading comprehension. Features more challenging tests. Both GLUE and SuperGLUE offer leaderboards to track and compare model performance. These benchmarks highlight that while models may reach human-level performance on specific tests, their general abilities still fall short of human expertise. MMLU (Massive Multitask Language Understanding): Tests modern LLMs on diverse topics such as mathematics, history, law, and computer science. Emphasizes advanced world knowledge and problem-solving. BIG-bench: Includes 204 tasks across disciplines like linguistics, common sense reasoning, software development, and biology. Offers scalability with multiple size options to manage costs. HELM (Holistic Evaluation of Language Models): Focuses on model transparency and suitability for specific tasks. Measures seven metrics (including fairness, bias, and toxicity) across 16 scenarios. Goes beyond accuracy with multidimensional evaluation to reveal trade-offs. HELM continuously evolves with new models, metrics, and scenarios, making it a dynamic resource for tracking progress in LLM capabilities. 2.4.2 Summary: Modern LLM benchmarks like GLUE, SuperGLUE, MMLU, BIG-bench, and HELM provide essential tools for evaluating language models. They offer valuable insights into models‚Äô strengths, weaknesses, and performance across a variety of tasks and contexts, aiding both research and practical applications. "],["fine-tuning.html", "Chapter 3 Fine Tuning 3.1 Intro 3.2 Fine-Tuning Overview 3.3 Fine-tuning of large language models (LLMs) and the associated challenges 3.4 Multitask Fine-Tuning 3.5 Evaluating the performance of large language models", " Chapter 3 Fine Tuning 3.1 Intro Instruction Fine-Tuning Instruction fine-tuning adjusts a pre-trained model to better respond to specific prompts and tasks. It is a significant advancement, allowing models trained on vast datasets to learn how to follow instructions effectively. Challenges in Fine-Tuning Catastrophic forgetting can occur, where the model loses previously learned information during fine-tuning. Techniques to mitigate this include using a diverse range of instruction types during the fine-tuning process. Parameter Efficient Fine-Tuning (PEFT) PEFT methods allow for fine-tuning without adjusting all model parameters, reducing computational and memory costs. Techniques like LoRA (Low-Rank Adaptation) enable effective fine-tuning with minimal resource requirements, making it accessible for various applications. 3.2 Fine-Tuning Overview Fine-tuning is a supervised learning process that updates the weights of a base model using a dataset of labeled examples. Instruction fine-tuning is a method that improves a model‚Äôs performance across various tasks by training it with examples that demonstrate how to respond to specific instructions. 3.2.1 Preparing Training Data To fine-tune a model, you need to prepare a dataset of prompt-completion pairs, which includes instructions relevant to the task. Prompt template libraries can help convert existing datasets into instruction prompt datasets suitable for fine-tuning. 3.2.2 Fine-Tuning Process The fine-tuning process involves selecting prompts from the training dataset, generating completions with the LLM, and comparing these completions to the expected responses. The model‚Äôs weights are updated based on the calculated loss between the generated output and the training labels, improving its performance over multiple iterations. 3.2.3 Evaluation and Results After fine-tuning, the model‚Äôs performance is evaluated using validation and test datasets to measure accuracy. The outcome is a new version of the model, often referred to as an instruct model, which is better suited for the specific tasks you are interested in. 3.3 Fine-tuning of large language models (LLMs) and the associated challenges 3.3.1 Fine-Tuning for Specific Tasks Fine-tuning a pre-trained model can significantly improve its performance on a specific task, such as summarization, using a limited dataset of 500-1,000 examples. A downside is catastrophic forgetting, where the model loses its ability to perform other tasks it was trained on. 3.3.2 Strategies to Avoid Catastrophic Forgetting Determine if catastrophic forgetting impacts your application; if only one task is needed, it may not be an issue. For maintaining multitask capabilities, consider fine-tuning on multiple tasks at once, which requires a larger dataset of 50-100,000 examples. 3.3.3 Parameter Efficient Fine-Tuning (PEFT) PEFT techniques focus on preserving the original model‚Äôs weights while training a small number of task-specific parameters, which helps mitigate catastrophic forgetting. This area is actively researched and will be discussed in more detail later in the course. 3.4 Multitask Fine-Tuning It involves training a model on a dataset that includes multiple tasks, such as summarization, review rating, code translation, and entity recognition. This approach helps improve the model‚Äôs performance across all tasks simultaneously and mitigates the issue of catastrophic forgetting. 3.4.1 FLAN Models The FLAN family of models, which stands for fine-tuned language net, is an example of models trained using multitask instruction fine-tuning. FLAN-T5 and FLAN-PALM are specific instruct versions of the T5 and PALM foundation models, respectively, fine-tuned on numerous datasets. 3.4.2 Example Dataset: SAMSum SAMSum is a dataset used for training models to summarize dialogues, containing 16,000 conversations with human-crafted summaries. The dataset is designed to reflect real-life messenger conversations, ensuring high-quality training data for language models. 3.4.3 Fine-Tuning for Specific Use Cases Additional fine-tuning can be performed on models like FLAN-T5 using domain-specific datasets, such as dialogsum, to improve performance in particular contexts, like customer service chats. The importance of using internal company data for fine-tuning is emphasized to tailor the model to specific summarization needs. 3.4.4 Evaluation of Model Performance The content highlights the need to evaluate the quality of model outputs after fine-tuning, which will be discussed in the next video. 3.4.5 What is the purpose of fine-tuning with prompt datasets? To improve the performance and adaptability of a pre-trained language model for specific tasks. This option accurately describes the purpose of fine-tuning with prompt datasets. It aims to improve the performance and adaptability of a pre-trained language model by training it on specific tasks using instruction prompts. 3.5 Evaluating the performance of large language models Since, LLM are probabilictic and there are millions of results to evaluate, human evaluation is not possible and requires a systematic solution. 3.5.1 Evaluation Metrics ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is used to assess the quality of automatically generated summaries by comparing them to human-generated references. BLEU (Bilingual Evaluation Understudy) evaluates the quality of machine-translated text by comparing n-grams in the generated translation to those in a reference translation. 3.5.2 Understanding ROUGE ROUGE-1 measures unigram matches (single words) between generated and reference sentences, calculating recall, precision, and F1 scores. ROUGE-2 extends this by considering bigrams (pairs of words), which helps account for word order. 3.5.3 Limitations and Improvements Simple ROUGE scores can be misleading, as they may yield high scores for poor outputs. Clipping functions can be used to limit unigram matches to their maximum count in the reference. The ROUGE-L score uses the longest common subsequence to provide a more nuanced evaluation. 3.5.4 Using BLEU Score The BLEU score averages precision across multiple n-gram sizes, providing a comprehensive measure of translation quality. Both ROUGE and BLEU are low-cost metrics useful for iterative model evaluation, but should not be the sole indicators of model performance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
