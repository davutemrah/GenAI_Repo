[["index.html", "Generative AI with Large Language Models Introduction", " Generative AI with Large Language Models DEA 2025-01-10 Introduction "],["introduction-1.html", "Chapter 1 Introduction 1.1 Generative AI Overview 1.2 Use Cases of LLM 1.3 Text Generation before Transformers 1.4 Introduction of Transformer Architecture 1.5 Transformers 1.6 Transformer Architecture 1.7 Some well-known models 1.8 Prompt Engineering 1.9 Configuring Generative AI Models 1.10 Generative AI project lifecycle 1.11 Pre-training large language models", " Chapter 1 Introduction 1.1 Generative AI Overview Generative AI tools have become widely accessible and are capable of creating content that mimics or approximates human ability. Examples include: Chatbots Image generation from text Code development plugins 1.1.1 What is Generative AI? Subset of Traditional Machine Learning: Generative AI models learn by finding statistical patterns in massive datasets originally created by humans. Training: Large language models (LLMs) are trained on trillions of words over many weeks or months using significant computational power. Foundation Models: These models, sometimes called base models, have billions of parameters and exhibit emergent properties beyond language, including reasoning and problem-solving abilities. Bert, GPT, LLaMa, BLOOM, PaLM, FLAN-T5 1.1.2 Understanding Parameters Parameters as Memory: Think of parameters as the model’s memory. More parameters mean more memory, which typically leads to the ability to perform more sophisticated tasks. Model Representation: Throughout the course, LLMs will be represented by purple circles. Example Model: In the labs, you will use an open-source model called flan-T5 for language tasks. 1.1.3 Model Adaptation and Use Fine Tuning: You can either use these models as they are or apply fine-tuning techniques to adapt them to specific use cases without the need to train a new model from scratch. Multimodal Generative AI: While generative AI models exist for images, video, audio, and speech, this course focuses on LLMs and natural language generation. 1.1.4 Interaction with LLMs Natural Language Prompts: Unlike traditional machine learning and programming paradigms that use formalized syntax, LLMs take natural language instructions (prompts) and perform tasks. Context Window: The space available for a prompt is called the context window, typically large enough for a few thousand words. Prompt –&gt; LLM –&gt; Completion (generated text) 1.2 Use Cases of LLM The broader applications of large language models (LLMs) and generative AI beyond chatbots. It highlights that while chatbots are prominent, LLMs can also perform diverse tasks, such as: Text Generation: LLMs can generate essays based on prompts and summarize dialogues. Translation: They can translate between different languages and convert natural language into machine code, such as generating Python code. Information Retrieval: LLMs can identify named entities (people and places) in texts through tasks like named entity recognition. Augmentation: There’s ongoing development in connecting LLMs to external data sources and APIs to enhance their capabilities and access real-time information. Larger models (with billions of parameters) show improved language understanding, although smaller models can be fine-tuned for specific tasks. The rapid advancements in LLM capabilities are attributed to their underlying architecture. 1.3 Text Generation before Transformers Introduction to Generative Algorithms Generative algorithms have been a part of natural language processing for a long time. Before the advent of transformer models, the primary architecture used for generative tasks was recurrent neural networks (RNNs). 1.3.1 Recurrent Neural Networks (RNNs) Architecture: RNNs process sequences of data by maintaining a hidden state that captures information from previous time steps. Limitations: Computational and Memory Constraints: RNNs require significant computational resources and memory to process long sequences. Short-Term Memory: RNNs struggle with long-term dependencies due to vanishing gradients, leading to poor performance on tasks requiring an understanding of extended context. 1.3.2 Example of RNN in Action Simple Next-Word Prediction Task: With only one preceding word, the RNN’s prediction is not very accurate. Scaling the RNN to consider more preceding words increases computational complexity and resource usage. Despite scaling, the model often fails to capture enough context for accurate predictions. 1.3.3 Challenges in Language Understanding Context Requirement: Successful next-word prediction requires understanding the entire sentence or document. Complexity of Language: Homonyms: Words with multiple meanings depending on context (e.g., “bank” as a financial institution or the side of a river). Syntactic Ambiguity: Sentence structures can be ambiguous (e.g., “The teacher taught the students with the book” – did the teacher use the book or did the students have the book?). 1.4 Introduction of Transformer Architecture In 2017, the transformer architecture revolutionized generative AI with the publication of the paper “Attention is All You Need” by researchers from Google and the University of Toronto. 1.4.1 Advantages of Transformers Efficient Scaling: Can be efficiently scaled to use multi-core GPUs. Parallel Processing: Processes input data in parallel, allowing for the use of larger training datasets. Attention Mechanism: Learns to pay attention to the meaning of words in context. Addresses the limitations of RNNs by considering the relevance of each word in the input sequence to every other word. The transformer architecture marked a significant breakthrough in natural language processing, enabling models to handle complex generative tasks more efficiently and accurately. The title of the influential paper, “Attention is All You Need,” underscores the importance of the attention mechanism in transforming the field of generative AI. 1.5 Transformers 1.5.1 Overview The transformer architecture significantly improved natural language processing tasks compared to earlier RNNs, enabling superior generative capabilities. Its power lies in learning the relevance and context of words across a sentence using attention mechanisms. 1.5.2 Attention Mechanisms Self-Attention: This mechanism learns the relationships between all words in a sentence, allowing the model to understand the context and relevance of each word in relation to others. Attention Map: A visual representation of the attention weights, showing how words relate to each other. For example, the word “book” might strongly connect with “teacher” and “student”. 1.5.3 Transformer Architecture The transformer model consists of two main components: Encoder: Encodes input sequences into deep representations. Decoder: Uses these representations to generate output sequences. 1.5.4 Tokenization Converts words into numbers representing their positions in a dictionary. Tokenization methods can vary, representing whole words or parts of words. Consistent tokenization is essential for both training and generating text. 1.5.5 Embedding Layer Transforms token IDs into high-dimensional vectors. Encodes the meaning and context of tokens in a vector space. 1.5.6 Positional Encoding Adds information about the position of words in a sentence, preserving word order relevance. 1.5.7 Self-Attention Layer Analyzes relationships between tokens in the input sequence. Multi-headed self-attention means multiple sets of self-attention weights are learned in parallel. Each attention head learns different aspects of language, enhancing contextual understanding. 1.5.8 Feed-Forward Network Processes outputs from the self-attention layer. Produces logits proportional to the probability of each token in the dictionary. 1.5.9 Softmax Layer Normalizes logits into probability scores for each token. The highest probability token is selected as the next word in the sequence. 1.5.10 Prediction Process Let’s walk through a sequence-to-sequence task, such as translating a French phrase into English: Tokenize Input: The French phrase is tokenized using the same tokenizer that trained the network. Encoder: Tokenized input is passed through the embedding layer and multi-headed attention layers, producing a deep representation of the input sequence. Decoder: A start-of-sequence token triggers the decoder to predict the next token. The decoder uses the encoder’s contextual understanding to generate the output token. Loop: The output token is fed back into the decoder to predict the next token until an end-of-sequence token is predicted. Detokenization: The sequence of tokens is converted back into words to form the final output. 1.5.11 Example Input: French phrase “Je t’aime la machine d’apprentissage”. Tokenized input is processed through the encoder and decoder. Output: “I love machine learning”. 1.6 Transformer Architecture 1.6.1 Overview The transformer architecture consists of encoder and decoder components, essential for various natural language processing tasks. 1.6.2 Encoder Function: Encodes input sequences into a deep representation of their structure and meaning. Usage: You can train encoder-only models to perform classification tasks such as sentiment analysis Encoder-only models, such as BERT, work as sequence-to-sequence models with equal input and output sequence lengths. With additional layers, you can train encoder-only models to perform classification tasks like sentiment analysis. 1.6.3 Decoder Function: Uses the encoder’s contextual understanding to generate new tokens, operating in a loop until a stop condition is met. Usage: Decoder-only models, such as the GPT family, BLOOM, Jurassic, and LLaMA, are commonly used today and can generalize to most tasks. 1.6.4 Encoder-Decoder Models Function: Handle sequence-to-sequence tasks where input and output sequences can differ in length. Examples: BART, T5. Usage: Suitable for tasks like translation and general text generation. 1.6.5 Practical Application Main Goal: Understand the differences between various models to read model documentation effectively. Prompt Engineering: Interact with transformer models through natural language prompts, focusing on written words rather than code. Transformers: Attention is all you need “Attention is All You Need” is a research paper published in 2017 by Google researchers, which introduced the Transformer model, a novel architecture that revolutionized the field of natural language processing (NLP) and became the basis for the LLMs we now know - such as GPT, PaLM and others. The paper proposes a neural network architecture that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with an entirely attention-based mechanism. The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively. The authors demonstrate that their model achieves state-of-the-art performance on several machine translation tasks and outperforms previous models that rely on RNNs or CNNs. The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network applies a point-wise fully connected layer to each position separately and identically. The Transformer model also uses residual connections and layer normalization to facilitate training and prevent overfitting. In addition, the authors introduce a positional encoding scheme that encodes the position of each token in the input sequence, enabling the model to capture the order of the sequence without the need for recurrent or convolutional operations. 1.7 Some well-known models 1.7.1 BERT BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model developed by Google in 2018, designed specifically for natural language understanding (NLU) tasks. BERT differs from earlier models because of its bidirectional training approach, which allows it to understand context in a nuanced way by considering both the left and right context in a sentence. 1.7.1.1 Key Details About BERT: Parameter Sizes: BERT Base: 110 million parameters (12 layers, 768 hidden units, 12 attention heads) BERT Large: 340 million parameters (24 layers, 1024 hidden units, 16 attention heads) Bidirectional Training: BERT’s bidirectional approach uses masked language modeling (MLM) during training. This means it randomly masks words in a sentence and trains the model to predict them based on the surrounding context, giving it a deeper understanding of word relationships. Pretraining Tasks: Masked Language Modeling (MLM): BERT masks a portion of the words in a sentence and tries to predict the missing words. Next Sentence Prediction (NSP): BERT also learns to predict if one sentence logically follows another, which helps with tasks like question answering and sentence-pair classification. Fine-Tuning for Specific Tasks: BERT’s architecture is flexible, allowing it to be fine-tuned for a range of NLP tasks with minimal additional training. Common tasks include: Sentiment Analysis Named Entity Recognition (NER) Question Answering (e.g., SQuAD benchmark) Text Classification and Similarity Tasks Impact and Variants: BERT set a new benchmark in NLP, and its architecture has inspired many derivatives and improvements, including RoBERTa (by Facebook AI), DistilBERT (a smaller, faster BERT), and ALBERT (a more efficient BERT variant). BERT transformed NLP by demonstrating how powerful bidirectional transformers can be for understanding language context, which has since influenced other models like GPT, T5, and BLOOM. 1.7.2 BLOOM BLOOM (BigScience Large Open-science Open-access Multilingual Language Model) is a large language model created by the BigScience project, an open scientific collaboration led by Hugging Face with researchers worldwide. BLOOM is designed to handle multiple languages and tasks, such as text generation, summarization, and translation, in a way similar to models like GPT-3. 1.7.2.1 Key Details About BLOOM: Parameter Size: BLOOM has 176 billion parameters in its largest version, making it one of the largest open-access language models available. There are smaller versions with fewer parameters, which are used for different applications requiring less computational power. Multilingual Capacity: BLOOM was trained on 46 languages and 13 programming languages, including English, French, Arabic, Spanish, Chinese, and more, making it versatile in multilingual NLP tasks. Training Data: The model was trained on a vast, diverse dataset of over 1.5 terabytes of text data, which includes a variety of domains to help BLOOM perform across multiple contexts and languages. Open Access: One of the primary goals of the BLOOM model is transparency and open accessibility for the AI research community, unlike other large models that are proprietary. If you’re working in NLP or language modeling, BLOOM is an excellent model to explore due to its openness, multilingual capabilities, and state-of-the-art performance across various tasks. 1.8 Prompt Engineering Introduction Prompt engineering involves crafting and refining the input text (prompt) fed to a model to influence its behavior during inference, resulting in desired output (completion). The total text available for the prompt is called the context window. 1.8.1 Key Concepts Prompt Engineering Definition: The process of developing and improving the prompt to achieve desired model behavior. Strategy: Including examples of the task within the prompt, known as in-context learning. In-Context Learning Definition: Helping models learn the task by including examples in the prompt. Example: For sentiment analysis, the prompt can include the instruction, review text, and an expected sentiment output. Zero-Shot Inference Definition: Providing only the input data in the prompt without examples. Effectiveness: Large models perform well; smaller models may struggle. One-Shot Inference Definition: Including a single example in the prompt to guide the model. Example: A sample review and sentiment analysis followed by the actual input review. Few-Shot Inference Definition: Including multiple examples in the prompt to improve model understanding. Example: A mix of positive and negative reviews to guide sentiment analysis. Practical Considerations Context Window Limitation: There’s a limit on the amount of in-context learning that can be included. Recommendation: If performance doesn’t improve with multiple examples, consider fine-tuning the model. Fine-Tuning Definition: Additional training on the model with new data to improve task-specific performance. Upcoming: Detailed exploration of fine-tuning will be covered in week 2 of the course. Model Performance and Scale Observation: Model performance on various tasks depends on the scale (number of parameters). Large Models: Good at zero-shot inference for multiple tasks. Smaller Models: Generally limited to tasks similar to their training data. Experimentation Recommendation: Experiment with different models and settings to find the best fit for your use case. Next Steps: Explore configuration settings to influence the structure and style of model completions. 1.9 Configuring Generative AI Models Overview This lecture examines methods and configuration parameters used to influence a model’s next-word generation during inference. Unlike training parameters, these settings are adjusted at inference time to control aspects like the maximum number of tokens generated and the creativity of the output. Key Configuration Parameters 1. Max New Tokens Definition: Limits the number of tokens the model generates. Usage: Set to values like 100, 150, or 200 to cap the generation process. Example: If set to 200, the generation might end sooner if an end-of-sequence token is predicted. 2. Greedy Decoding Definition: The model always selects the word with the highest probability. Characteristics: Works well for short texts but can lead to repetitive outputs. 3. Random Sampling Definition: Selects the next word based on the probability distribution. Advantages: Introduces variability to avoid repetitive text. Disadvantages: Can produce outputs that are too creative or nonsensical. Implementation: In Hugging Face, set do_sample=True. 4. Top-k Sampling Definition: Limits choices to the top k highest probability tokens. Example: If k=3, the model selects from the top 3 probable words. Benefit: Balances randomness and coherence in the output. 5. Top-p (Nucleus) Sampling Definition: Chooses from tokens whose cumulative probability meets a threshold p. Example: If p=0.3, selects from tokens that together have a probability of 0.3. Benefit: Ensures sensible and coherent generation by limiting low-probability words. 6. Temperature Definition: Controls the randomness by scaling the probability distribution. Effect: Low temperature (&lt;1): Concentrates probability on fewer words, producing less random and more predictable text. High temperature (&gt;1): Spreads probability more evenly, increasing randomness and creativity. Temperature=1: Uses the default probability distribution. 1.10 Generative AI project lifecycle Introduction Throughout this course, you’ll learn the techniques required to develop and deploy an LLM-powered application. This video introduces the generative AI project life cycle, guiding you from conception to launch. 1.10.1 Project Life Cycle Stages 1.10.1.1 Define the Scope Importance: Accurately and narrowly define the project’s scope. Considerations: What specific function will the LLM serve in your application? Does the model need to perform various tasks or focus on one specific task (e.g., named entity recognition)? Outcome: Save time and compute costs by clearly defining requirements. 1.10.1.2 Choose a Model Decision: Train your own model from scratch or use an existing base model. Common Approach: Start with an existing model. Considerations: Later in the course, you’ll learn rules of thumb to help estimate the feasibility of training your own model. 1.10.1.3 Assess and Train the Model Initial Steps: Use prompt engineering and in-context learning to improve model performance. Fine-Tuning: Necessary if prompt engineering is insufficient. Supervised learning process covered in Week 2. Reinforcement Learning with Human Feedback (RLHF): Ensures the model behaves well and aligns with human preferences. - Covered in Week 3. 1.10.1.4 Evaluate the Model Metrics and Benchmarks: Explore next week. Iterative Process: Start with prompt engineering. Evaluate outputs and fine-tune if necessary. Revisit and refine prompt engineering. 1.10.1.5 Deployment Final Steps: Optimize the model for deployment. Integrate the model with your application. Ensure efficient use of compute resources for a better user experience. 1.10.1.6 Additional Infrastructure Considerations Limitations of LLMs: Tendency to invent information when unsure. Limited ability to perform complex reasoning and mathematics. Overcoming Limitations: Learn techniques to address these issues in the final part of the course. 1.10.1.7 Conclusion This framework maps out the tasks required to take your project from conception to launch, providing insights into important decisions, potential difficulties, and necessary infrastructure for developing and deploying your application. 1.11 Pre-training large language models Generative AI Project Life Cycle Overview The process begins with scoping the use case and determining how an LLM will integrate into the application. Next, choose a model: either use an existing model or train one from scratch (the latter is explored in detail later). Choosing a Model Start with existing foundation models available on hubs like Hugging Face or PyTorch, which include model cards detailing use cases, training processes, and limitations. The model choice depends on the task and the architecture’s strengths. Transformer Model Variants Encoder-Only Models (Autoencoding Models): Pre-trained with masked language modeling to reconstruct input by predicting masked tokens. Suited for sentence classification (e.g., sentiment analysis) and token classification (e.g., named entity recognition). Examples: BERT, RoBERTa. Decoder-Only Models (Autoregressive Models): Pre-trained with causal language modeling to predict the next token. Best for text generation and capable of zero-shot inference. Examples: GPT, BLOOM. Sequence-to-Sequence Models: Utilize both encoder and decoder components. Pre-trained with tasks like span corruption (e.g., T5) or other methods. Ideal for translation, summarization, and question answering. Examples: T5, BART. Pre-Training Process LLMs are trained on large datasets using self-supervised learning to capture language patterns. Data preprocessing ensures quality and reduces bias, with only 1–3% of tokens retained for training. Large-scale compute resources and GPUs are required. Model Size and Performance Larger models exhibit better performance with reduced need for in-context learning or further training. Advances in transformers, data availability, and compute power have driven the creation of increasingly larger models. However, training large models is expensive and resource-intensive, raising challenges in scaling. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
